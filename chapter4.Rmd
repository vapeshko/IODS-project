# Applying clustering methods to Boston data set


## Overview of the data

For this task I am using *Housing Values in Suburbs of Boston*, which is included into MASS package. The data set consists of 506 observations and 14 variables. See more information about the data [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).

Summary of the variables reveals a few striking characteristics of Bostos suburbs. Per capita crime rate `crim` shows that majority of suburbs have relatively low crime rate with a few outliers. The measure  `black` reveals that majority of suburbs has similar proportions of black population but there is a group of suburbs with very amount of black residents.

Correlation plots reveals that crime rate is strongly associated with accessibility of radial highways, `rad`,  with full value property tax rate `tax`. These variables also highly dependent from each other. Relatively strong negative relationships are visible with distances to employment centers (`dis`), proportion of black residents (`black`), and median value of owner-occupied homes (`medv`). Suburbs with high grime rates also seem to located near industrial centers (`indus`) and,prbably as a result, be more polluted (`nox`).

```{r}
library(MASS)
library(dplyr)
library(tidyr)
library(corrplot)
library(ggplot2)

data("Boston")

# structure and dimensions of the data
glimpse(Boston)

# summaries of variables
summary(Boston)

# graphical overview

cor_matrix<-cor(Boston) %>% round(digits = 2)

## measuring statistical significance of correlation coefficients
testRes <- cor.mtest(cor_matrix, conf.level = 0.95)

## plotting with significant level stars
corrplot(cor_matrix, p.mat = testRes$p, method="square", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6, sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, insig = 'label_sig', pch.col = 'black')

```

## Preparing the data set for future analysis

### Scaling

For the purposes of lda analysis, I am first scaling the data set. As you can see, now the means of all variables are zero, so it is easy now to compare their "intensity".

```{r}
# centering and standardizing variables
boston_scaled <- scale(Boston)

boston_scaled <- as.data.frame(boston_scaled)

summary(boston_scaled)

```

### Creating crime rate target variable

In order conduct lda analysis with crime rate as a target variable, I will need to transform it to a categorical data. I am going to create categories based on quantiles, but as the summary of this variable has indicated earlier, I am expecting to see a significant break in the third quantile.

```{r}
# creating a quantile vector
bins <- quantile(boston_scaled$crim)

# creating a categorical variable 'crime'
values <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, label = values, include.lowest = TRUE)

# removing original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# adding the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

summary(boston_scaled$crime)

```

### Dividing to train and test sets

Finally, I am dividing the set, allocating 80% as train data, and the rest as test data.

```{r}
# number of rows in the Boston data set 
n <- nrow(boston_scaled)

# selecting randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# train set
train <- boston_scaled[ind,]

# test set 
test <- boston_scaled[-ind,]

# saving the correct classes from test data
correct_classes <- test$crime

# removing the crime variable from test data
test <- dplyr::select(test, -crime)
```

## LDA analysis

### Fitting the model

Now the data set is ready to be fitted to LDA with crime rate as the target variable. The result is visualized with a biplot. It is clear from the plot that my initial hypothesis was correct, the cases are divided in two distinct clusters: a bigger one, consisting of all low and med_low cases, majority of med_high and a smaller one with majority of high cases and several med_high. Arrows make it clear that the strongest factor dividing the clusters seems to be accessibility of radial highways, `rad`.

```{r}

lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plotting the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.5)
```

### Testing the model

Test of the model's prediction capacity has revealed that it is rather accurate. It has correctly predicted all high cases, 78% of med_high and 68% of low. The model seems performs worse in the med_low category with only 41% of correct predictions.

```{r}
lda.pred <- predict(lda.fit, newdata = test)

table(correct = correct_classes, predicted = lda.pred$class)

```
## K-means clustering

I will try to use another clustering method on Boston data -- K-means clustering.

### Scaling the data set

```{r}
# reloading data
data("Boston")

# scaling
Boston_scaled <- scale(Boston)

Boston_scaled <- as.data.frame(Boston_scaled)

```

### Calculating distances

```{r}
# Euclidean distance
dist_eu <- dist(Boston_scaled)

# Manhattan distance
dist_man <- dist(Boston_scaled, method = "manhattan")

# summaries
summary(dist_eu)
summary(dist_man)

```
### K means clustering

I will first try running k-means algorithm with 4 clusters, as our basic classification was based on quanttiles. From plots of `rad`` and `tax`, we can again see blue group separated from the other three color points, so it seema that anather k value will give better results.

```{r}
km <-kmeans(Boston_scaled, centers = 4)

pairs(Boston_scaled, col = km$cluster)

```

I will determine the oprimal number of clusters by plotting total of within cluster sum of squares by number of cluster (with maximum k = 10). We can see that the first "elbow" of the plot at k=2. Running k-means algorithm with two centers indeed gives a clear picture of two clusters.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston_scaled, col = km$cluster)
```

## Bonus

Here I am fitting lda model with k-means clusters as a target variable. The bi-plot again shows separation of our three k-clusters in two distict groups. `indus`, `nox`, `tax` and `rad` seem to pull the two groups apart, that means that Boston suburbs could be separated by proportion of industries located in the area, pollution level, full-value property-tax rate and accessibility of highways. Crime level doesn't seem to be at play.  

```{r}
# reloading data
data("Boston")

# scaling
boston_scale <- scale(Boston)
boston_scale <- as.data.frame(boston_scale)

# somehow knitting doesn't work with this varible
boston_scale <- dplyr::select(boston_scale, -chas)

set.seed(1234)

# running K-means algorithm
km <-kmeans(boston_scale, centers = 3)

# adding new clusters as target variable
clusters <- as.factor(km$cluster)

boston_scale <- mutate(boston_scale, clusters = clusters)

# running LDA
lda.fit2 <- lda(clusters ~ ., data = boston_scale)

# plotting the lda results
class <- as.numeric(boston_scale$clusters)

plot(lda.fit2, dimen = 2, col = class, pch = class)
lda.arrows(lda.fit2, myscale = 3)

```

